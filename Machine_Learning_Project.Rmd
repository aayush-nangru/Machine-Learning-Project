---
title: "Machine Learning Project"
author: "Aayush Nangru"
date: "Monday, December 22, 2014"
output: html_document
---


For this assignment, predictions will be made using two models - decision trees and random forests. The model with the highest accuracy, or the least out-of-sample error will be chosen as the final model.   


**Loading libraries and setting the seed**
```{r}
library(caret)
library(randomForest)   # Random forest for classification and regression 
library(rpart)          # Regressive Partitioning and Regression trees 
set.seed(10)
```



**Cross-validation**

Cross-validation was performed by subsampling our training data set randomly without replacement into 2 subsamples: subTraining data (75% of the original Training data set) and subTesting data (25%). The models will be fitted on the subTraining data set, and tested on the subTesting data. Once the most accurate model is choosen, it will be tested on the original Testing data set.

```{r}
# Loading the training data set into my R session replacing all missing with "NA"
trainingset <- read.csv("C:/Users/aayush_nangru/Documents/COURSE_ERA/Machine Learning/Project/pml-training.csv", na.strings=c("NA","#DIV/0!", ""))

# Loading the testing data set 
testingset <- read.csv("C:/Users/aayush_nangru/Documents/COURSE_ERA/Machine Learning/Project/pml-testing.csv", na.strings=c("NA","#DIV/0!", ""))

# Check dimensions for number of variables and number of observations
dim(trainingset)
dim(testingset)

# Delete columns with all missing values
trainingset<-trainingset[,colSums(is.na(trainingset)) == 0]
testingset <-testingset[,colSums(is.na(testingset)) == 0]

# Some variables are irrelevant to our current project: user_name, raw_timestamp_part_1, raw_timestamp_part_,2 cvtd_timestamp, new_window, and  num_window (columns 1 to 7). We can delete these variables.
trainingset   <-trainingset[,-c(1:7)]
testingset <-testingset[,-c(1:7)]

# In order to perform cross-validation, the training data set is partionned into 2 sets: subTraining (75%) and subTest (25%).This will be performed using random subsampling without replacement.
subsamples <- createDataPartition(y=trainingset$classe, p=0.75, list=FALSE)
subTraining <- trainingset[subsamples, ] 
subTesting <- trainingset[-subsamples, ]
dim(subTraining)
dim(subTesting)
```



**First Model**
```{r}
# First prediction model: Using Decision Tree
model1 <- rpart(classe ~ ., data=subTraining, method="class")

# Predicting:
prediction1 <- predict(model1, subTesting, type = "class")

# Test results on our subTesting data set:
confusionMatrix(prediction1, subTesting$classe)
```



**Second Model**
```{r}
# Second prediction model: Using Random Forest
model2 <- randomForest(classe ~. , data=subTraining, method="class")

# Predicting:
prediction2 <- predict(model2, subTesting, type = "class")

# Test results on subTesting data set:
confusionMatrix(prediction2, subTesting$classe)
```
 


**Out-of-Sample Error and Model Comparison**   

The expected out-of-sample error will correspond to the quantity: 1-accuracy in the cross-validation data. Accuracy is the proportion of correct classified observation over the total sample in the subTesting data set. Expected accuracy is the expected accuracy in the out-of-sample data set (i.e. original testing data set). Thus, the expected value of the out-of-sample error will correspond to the expected number of missclassified observations/total observations in the Test data set, which is the quantity: 1-accuracy found from the cross-validation data set.    
    
As per the results obtained, Random Forest algorithm performed better than Decision Trees.
Accuracy for Random Forest model was 0.9957, compared to 0.7624 for Decision Tree model. The random Forest model is thus choosen. The accuracy of the model is 0.995. The **expected out-of-sample error** is estimated at 0.005, or 0.5%, which is calculated as 1 - accuracy for predictions made against the cross-validation set.



**Submission**

```{r}
# predict outcome levels on the original Testing data set using Random Forest algorithm
predictfinal <- predict(model2, testingset, type="class")
predictfinal

# Write files for submission
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(predictfinal)
```

